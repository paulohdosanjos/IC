\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx} % Required for inserting images
\usepackage{parskip}
\graphicspath{ {./imagens/} }
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage{mathtools} %box inside align
\newcommand\cha{4}
\title{Capítulo \cha\   - Sutton}
\author{Paulo Henrique Albuquerque}
\date{2023-04-25}


\setlength{\parskip}{1ex} % espaçamento vertical entre parágrafos
\setlength{\parindent}{0pt} % recuo horizontal de parágrafos


\begin{document}

\maketitle

\section{Programação Dinâmica}
Algoritmos clássicos de programação dinâmica são de uso limitado pois eles assumem um modelo perfeito, além de terem um grande custo computacional.

Porém, os algoritmos de DP formam uma base essencial para entender outros métodos. Esses outros métodos geralmente tentam replicar os algoritmos de DP com menos computação e sem assumir um modelo ideal. O objetivo primário dos algoritmos de DP é computar funções valor.

\subsection{Avaliação de Política}
Consideramos o problema de calcular $v_{\pi}$ para um política $\pi$ arbitrária. Lembre-se de que,
\[
  v_{\pi}(s)=\sum_{a}^{} \pi(a|s) \sum_{s',r}^{} p(s',r|s,a)[r+\gamma v_{\pi}(s')]
.\] 

A equação acima representa um sistema linear de $|\mathcal{S}|$ equações lineares, que pode ser resolvido atráves de uma computação direta. Para os nossos propósitos, um método interativo é mais adequado. Considere uma sequência de aproximações para a função valor: $v_1,v_2,\ldots$. A primeira aproximação é escolhida de forma arbitrária (exceto para estados terminais, que devem ter valor 0), e cada aproximação sucessiva é obtida usando a equação de Bellman como uma regra de atualização:
\[
  v_{k+1}(s)=\mathbb{E}_{\pi}[R_{t+1}+\gamma v_{k}(S_{t+1})|S_t=s]=\sum_{a}^{} \pi(a|s) \sum_{s',r}^{} p(s',r,|s,a)[r+\gamma v_{k}(s')],
\] 
para todo $s \in \mathcal{S}$. Claramente, $v_{\pi}$ é um ponto fixo para essa regra de atualização, pois a equação de Bellman garante a igualdade nesse caso. De fato, a sequência $\{v_{k}\}$ converge para $v_{\pi}$ a medida que  $k \to \infty$ sob as mesmas condições que garantem a existência de $v_{\pi}$. Esse algorimo é chamado de \textit{avaliação iterativa de política}.

Na implementação do algoritmo podemos utilizar um array e atualizar os valores in place. Pode-se ser, então, que novos valores sejam usados em vez de valores antigos no lado direito da equação da regra de atualização. Essa algoritmo levemente modificado também funciona e, usualmente, converge mais rápido. Geralmente, utilizamos a versão in place quando pensamos em algoritmos de DP.

O algoritmo de avaliação iterativa de política é dado abaixo.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.4\textwidth]{fig4.1.png}
  \caption{Algoritmo de avaliação iterativa de política}
  \label{fig:}
\end{figure}

\newpage

\subsection{Melhoria de política}  
Para sabermos se uma política pode ser melhorada comparamos o valor de $v_{\pi}(s)$ com $q_{\pi}(s,a)$ para toda ação $a \in \mathcal{A}(s)$. Caso algum $q_{\pi}(s,a)$ seja maior que $v_{\pi}(s)$, a política pode ser melhorada dando preferência à ação $a$ quando o agente está no estado $s$. Anunciamos essa observação através do seguinte teorema.

Melhoria de política: Sejam $\pi$ e $\pi'$ duas políticas determinísticas tais que, para todo $s \in \mathcal{S}$,
\[
  q_{\pi}(s,\pi'(s)) \ge v_{\pi}(s)
.\] 

Então, a política $\pi'$ é tão boa quanto $\pi$, ou até melhor:
\[
  v_{\pi'}(s) \ge v_{\pi}(s)
,\] 
para todo $s \in \mathcal{S}$. Podemos estender essa argumentação para todos os estados. Então, para cada estado, procuramos a ação que maximize o retorno:
\[
  \pi'=\arg \max_{a} q_{\pi}(s,a)=\sum_{s',r}^{} p(s',r|s,a)[r+\gamma v_{\pi}(s')]
.\] 
O processo de construir uma nova política melhorada, ao fazê-la gulosa em relação às funções valor da política original, é chamado de \textit{melhoria de política}.

Suponha que ao construir uma nova política $\pi'$ através da melhoria de política, obtemos uma política não melhor que $\pi$. Ou seja, $v_{\pi'} = v_{\pi}$. Pela equação acima, segue que, para todo $s \in \mathcal{S}$:
\[
  v_{\pi}(s')=\max_{a} \sum_{s',r}^{} p(s',r|s,a)[r+\gamma v_{\pi'}(s')]
.\] 
Mas a equação acima é idêntica à equação de Bellman ótima. Ou seja, $v_{\pi'}=v_{\pi}=v_{\star}$. 

Até agora, consideramos políticas determinísticas. Ao considerarmos políticas estocásticas, basta fazermos a seguinte modificação natural:
\[
  q_{\pi}(s,\pi'(s))=\sum_{a}^{} \pi'(a|s)q_{\pi}(s,a)
.\] 

Além disso, se várias ações são maximizadoras, não precisamos selecionar somente uma ação. Cada ação dessas pode ser dada uma porção da probabilidade de ser selecionada na nova política gulosa. É claro que toda ação sub-maximal deve ter probabilidade zero.

\newpage

\subsection{Iteração de política}
A partir do momento que uma política $\pi$ foi melhorada usando $v_{\pi}$ para obter uma política melhorada $v_{\pi'}$, podemos então computar $v_{\pi'}$ e melhora-lá denovo para obter uma política ainda melhor $\pi''$. Podemos, portanto, obter uma sequência de políticas monotonicamente melhoradas e funções valor:
\[
  \pi_0 \xlongrightarrow[]{\text{E}} v_{\pi_0} \xlongrightarrow[]{\text{I}} \pi_1 \xlongrightarrow[]{\text{E}} v_{\pi_1} \xlongrightarrow[]{\text{I}} \pi_2 \xlongrightarrow[]{\text{E}} \ldots \xlongrightarrow[]{\text{I}} \pi_{\star} \xlongrightarrow[]{\text{E}} v_{\star} 
,\] 
onde $\xlongrightarrow[]{\text{E}}$ denota uma avaliação de política e $\xlongrightarrow[]{\text{I}}$ denota uma melhoria de política. Cada política é, garantidamente, uma melhoria estrita em relação à passada se não for uma política ótima. Para MDPs finitos, o processo converge para uma política ótima em um número finito de passos, visto que o número de políticas é finito. (Porque? Só para políticas determinísticas, não?). Veja o algoritmo abaixo.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.4\textwidth]{fig4.3.png}
  \caption{Algoritmo de iteração de políticas}
  \label{fig:fig4-3-png}
\end{figure}


Observe que há um bug. É possível que o algoritmo acima nunca termine, quando o processo fica alterando entre duas políticas igualmente boas (como isso é possível?). Note também que em cada avaliação de política, a função valor é inicializada com o valor da função da política anterior. Isso faz que o processo, em geral, convirga mais rapidamente.

\subsection{Jack's Car Rental}
Nessa seção, fazemos uma apresentação do problema \textit{Jack's Car Rental} junto com uma solução. A descrição do problema é a seguinte:

\begin{center}
\boxed{
\begin{minipage}{\textwidth}
\textit{Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and is credited 10 by the national company. If he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at a cost of 2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is , where is the expected number. Suppose is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be = 0.9 and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cars moved between the two locations overnight.}
\end{minipage}
}
\end{center}

\newpage

Para formular o problema como um MDP finito, definimos as seguintes variáveis aleatórias:

$X_t = $ "\textit{quantidade de carros na localização 1 ($L_1$) ao fim do dia $t$, após contabilizar as transferências de carros entre localizações na noite do dia anterior e os pedidos e devoluções ao longo do dia $t$.}"

$Y_t = $ "\textit{quantidade de carros na localização 2 ($L_2$) ao fim do dia $t$, após contabilizar as transferências de carros entre localizações na noite do dia anterior e os pedidos e devoluções ao longo do dia $t$.}"

$A_t$ = \textit{"quantidade de carros movidos ao final da noite do dia $t$ da localização 1 para a localização 2"}

Note que,
\[
-\min(y,MAX\_MOVES) \le A_t \le \min(x,MAX\_MOVES)
,\] 

onde $MAX\_MOVES = 5$ para essa instância do problema.


$P_t^{(1)}$ = "\textit{quantidade total de pedidos concretizados ao longo do dia $t+1$."}

O valor desse variável é limitado pela quantidade de carros disponíveis no início do dia $t+1$, $X_t-A_t$. Se  $\overline{P_t}^{(1)}$ é a quantidade de pedidos no dia $t+1$ (observe que essa quantidade pode ser maior que 20: é uma variável Poisson independente),
\[
P_t^{(1)} = \min(\overline{P_t}^{(1)}, X_t-A_t)
.\] 

Analogamente,
\[
P_t^{(2)} = \min(\overline{P_t}^{(2)}, Y_t+A_t)
.\] 
  $D_t^{(1)}$ = \textit{"quantidade total de devoluções concretizadas na localização 1 ao final do dia $t$."}

  $D_t^{(2)}$ = \textit{"quantidade total de devoluções concretizadas na localização 2 ao final do dia $t$."}

  Consideramos que o carros são devolvidos no final do dia, após terem sidos processados os pedidos naquele dia. Além disso, a quantidade final de carros na localização deve ser menor que $MAX\_CARS=20$. Então, se $\overline{D_t}^{(1)}$ é a quantidade de devoluções totais no dia $t+1$,
  \[
    D_t^{(1)} = \min(\overline{D_t}^{(1)},MAX\_CARS-(X_t-A_t-P_t^{(1)}))  
  .\] 

Analogamente,
\[
    D_t^{(2)} = \min(\overline{D_t}^{(2)},MAX\_CARS-(Y_t+A_t-P_t^{(2)}))  
.\] 

As variáveis $\overline{P_t}^{(1)}$, $\overline{P_t}^{(2)}$, $\overline{D_t}^{(1)}$ e $\overline{D_t}^{(2)}$ seguem distribuições de Poisson bem definidas, conforme a descrição do problema.

$R_t$ = \textit{"recompensa ao final do dia $t$, contabilizando o custo de transferências de carros da noite do dia anterior $t$ e o lucro devido aos pedidos concretizados ao longo do dia $t+1$."}

Dadas essas definições, podemos escrever as seguintes relações.
\[
  X_{t+1} = X_t - A_t + D_t^{(1)} - P_t^{(1)}
\] 
\[
  Y_{t+1} = Y_t + A_t + D_t^{(2)} - P_t^{(2)}
\] 
\[
  R_{t+1} = -2|A_t| + 10(P_t^{(1)} + P_t^{(2)})
\] 

Agora, descrevemos o MDP arcaboço para o problema. O estado do MDP é o par $S_t = (X_t,Y_t)$. Então, o conjunto de estados $S$ é dado por
\[
  S = \{(x,y) : x,y \in [0,MAX\_CARS]\}
.\] 


\end{document}
